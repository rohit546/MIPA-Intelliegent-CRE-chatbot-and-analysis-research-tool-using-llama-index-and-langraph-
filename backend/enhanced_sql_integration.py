"""
Integration module for GPT-4 SQL Generator with Feedback Loop
Connects the existing app.py pipeline with the self-correcting SQL module
"""

import os
from typing import Dict, Any, Optional, Tuple, List
from dataclasses import asdict
import json
import logging

from sql_feedback_loop import SQLFeedbackLoop, ValidationStatus
from query_parser import QueryParser, SQLGenerator

logger = logging.getLogger(__name__)

class EnhancedSQLGenerator:
    """Enhanced SQL generator with feedback loop integration"""
    
    def __init__(self, database_url: str, openai_api_key: str, max_iterations: int = 3):
        self.database_url = database_url
        self.openai_api_key = openai_api_key
        
        # Initialize feedback loop
        self.feedback_loop = SQLFeedbackLoop(database_url, max_iterations)
        
        # Initialize existing parser components
        self.query_parser = QueryParser()
        self.sql_generator = SQLGenerator()
        
        logger.info("EnhancedSQLGenerator initialized with feedback loop")
    
    def generate_and_validate_sql(self, user_query: str, gpt4_generated_query: str) -> Dict[str, Any]:
        """
        Main integration point - processes queries through the feedback loop
        
        Args:
            user_query: Natural language query from user
            gpt4_generated_query: SQL generated by GPT-4/LlamaIndex
            
        Returns:
            Enhanced response with validation, corrections, and learning metadata
        """
        logger.info(f"Processing enhanced query: {user_query[:100]}...")
        
        # Process through feedback loop
        feedback_result = self.feedback_loop.process_query(user_query, gpt4_generated_query)
        
        # Prepare enhanced response
        enhanced_response = {
            'original_query': gpt4_generated_query,
            'final_sql': feedback_result['final_query'],
            'results': feedback_result['result'],
            'validation_status': feedback_result['validation_status'].value,
            'was_corrected': feedback_result['validation_status'] == ValidationStatus.CORRECTED,
            'iteration_count': feedback_result['iteration_count'],
            'explanation': feedback_result['explanation'],
            'correction_history': feedback_result['correction_history'],
            'constraints': asdict(feedback_result['constraints']),
            'metadata': {
                'execution_time': feedback_result['result'].execution_time,
                'row_count': feedback_result['result'].row_count,
                'columns': feedback_result['result'].columns,
                'errors': feedback_result['result'].errors,
                'warnings': feedback_result['result'].warnings
            }
        }
        
        # Log performance metrics
        self._log_performance_metrics(enhanced_response)
        
        return enhanced_response
    
    def get_parser_fallback_sql(self, user_query: str) -> Optional[str]:
        """Generate fallback SQL using the existing parser when GPT-4 fails"""
        try:
            parsed_query = self.query_parser.parse(user_query)
            fallback_sql = self.sql_generator.generate(parsed_query)
            logger.info("Generated parser fallback SQL")
            return fallback_sql
        except Exception as e:
            logger.error(f"Parser fallback failed: {e}")
            return None
    
    def _log_performance_metrics(self, response: Dict[str, Any]):
        """Log performance and learning metrics"""
        metrics = {
            'validation_status': response['validation_status'],
            'was_corrected': response['was_corrected'],
            'iteration_count': response['iteration_count'],
            'execution_time': response['metadata']['execution_time'],
            'row_count': response['metadata']['row_count'],
            'has_errors': len(response['metadata']['errors']) > 0
        }
        
        logger.info(f"Query metrics: {json.dumps(metrics, indent=2)}")
    
    def get_learning_insights(self) -> Dict[str, Any]:
        """Get insights from the learning system"""
        return self.feedback_loop.get_learning_stats()

def integrate_with_existing_app(database_url: str, openai_api_key: str) -> EnhancedSQLGenerator:
    """Factory function to create integrated SQL generator"""
    return EnhancedSQLGenerator(database_url, openai_api_key)

# Decorator for existing query functions
def with_feedback_loop(enhanced_generator: EnhancedSQLGenerator):
    """Decorator to add feedback loop to existing query functions"""
    
    def decorator(original_query_func):
        def wrapper(user_query: str, *args, **kwargs):
            # Execute original function to get GPT-4 generated SQL
            original_response = original_query_func(user_query, *args, **kwargs)
            
            # Extract SQL from original response
            gpt4_sql = None
            if hasattr(original_response, 'metadata') and 'sql_query' in original_response.metadata:
                gpt4_sql = original_response.metadata['sql_query']
            
            if gpt4_sql:
                # Process through feedback loop
                enhanced_response = enhanced_generator.generate_and_validate_sql(user_query, gpt4_sql)
                
                # Update original response with enhanced data
                if hasattr(original_response, 'metadata'):
                    original_response.metadata.update({
                        'enhanced_sql': enhanced_response['final_sql'],
                        'validation_status': enhanced_response['validation_status'],
                        'was_corrected': enhanced_response['was_corrected'],
                        'correction_explanation': enhanced_response['explanation'],
                        'learning_metadata': enhanced_response['constraints']
                    })
                
                # Use corrected results if available
                if 'result' in enhanced_response and enhanced_response['results'].rows:
                    original_response.metadata['result'] = enhanced_response['results'].rows
                    original_response.metadata['sql_query'] = enhanced_response['final_sql']
            
            return original_response
        
        return wrapper
    return decorator

class FeedbackLoopReporter:
    """Reporting utilities for feedback loop performance"""
    
    def __init__(self, enhanced_generator: EnhancedSQLGenerator):
        self.enhanced_generator = enhanced_generator
    
    def generate_performance_report(self) -> str:
        """Generate a performance report"""
        stats = self.enhanced_generator.get_learning_insights()
        
        report = f"""
=== SQL Feedback Loop Performance Report ===

ðŸ“Š OVERALL STATISTICS:
â€¢ Total Queries Processed: {stats.get('total_records', 0)}
â€¢ Average Iterations per Query: {stats.get('average_iterations', 0)}

ðŸ“ˆ VALIDATION STATUS DISTRIBUTION:
"""
        
        status_dist = stats.get('status_distribution', {})
        for status, count in status_dist.items():
            percentage = (count / stats.get('total_records', 1)) * 100
            report += f"â€¢ {status.title()}: {count} queries ({percentage:.1f}%)\n"
        
        report += f"""
ðŸ”§ MOST COMMON CORRECTIONS:
"""
        
        common_corrections = stats.get('common_corrections', [])
        for i, (correction, count) in enumerate(common_corrections[:5], 1):
            report += f"{i}. {correction}: {count} occurrences\n"
        
        report += """
ðŸ’¡ INSIGHTS:
â€¢ Corrections help improve query accuracy over time
â€¢ Learning patterns reduce future correction needs
â€¢ Monitor correction frequency to identify systematic issues
"""
        
        return report
    
    def get_correction_recommendations(self) -> List[str]:
        """Get recommendations based on correction patterns"""
        stats = self.enhanced_generator.get_learning_insights()
        recommendations = []
        
        # Analyze common corrections
        common_corrections = stats.get('common_corrections', [])
        
        if any('county filter' in correction[0].lower() for correction in common_corrections):
            recommendations.append(
                "Consider improving GPT-4 prompts to use address->>'county' for county searches"
            )
        
        if any('aggregation' in correction[0].lower() for correction in common_corrections):
            recommendations.append(
                "Enhance aggregation query examples in GPT-4 system prompt"
            )
        
        avg_iterations = stats.get('average_iterations', 0)
        if avg_iterations > 1.5:
            recommendations.append(
                "High correction rate detected - review and update GPT-4 system prompts"
            )
        
        if not recommendations:
            recommendations.append("System is performing well - no specific improvements needed")
        
        return recommendations